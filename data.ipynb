{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNVn0xkf7-aB"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "def StoryDataLoader(fname, tokenizer, batch_size, max_length, mode=\"train\"):\n",
        "    \"\"\"\n",
        "    Build Data Loader\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = Dataset.from_json(fname, mode)\n",
        "\n",
        "    if not tokenizer.cls_token:\n",
        "        tokenizer.cls_token = tokenizer.bos_token\n",
        "    if not tokenizer.sep_token:\n",
        "        tokenizer.sep_token = tokenizer.eos_token\n",
        "\n",
        "    tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
        "        single=f\"{tokenizer.cls_token} $0 {tokenizer.sep_token}\",\n",
        "        pair=f\"{tokenizer.cls_token} $A {tokenizer.sep_token} $B:1 {tokenizer.sep_token}:1\",\n",
        "        special_tokens=[(tokenizer.cls_token, tokenizer.cls_token_id), (tokenizer.sep_token, tokenizer.sep_token_id)],\n",
        "    )\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        processed = {}\n",
        "        tokenizer_input = tokenizer(\n",
        "            examples[\"input\"][\"sentence1\"],\n",
        "            examples[\"input\"][\"sentence3\"],\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            truncation=True\n",
        "        )\n",
        "        processed[\"input_ids\"] = tokenizer_input[\"input_ids\"],\n",
        "        processed[\"attention_mask\"] = tokenizer_input[\"attention_mask\"]\n",
        "\n",
        "        if mode == \"train\":\n",
        "            tokenizer_output = tokenizer(\n",
        "                examples[\"output\"],\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_length,\n",
        "                truncation=True\n",
        "            )\n",
        "            processed[\"decoder_input_ids\"] = tokenizer_output[\"input_ids\"]\n",
        "            processed[\"decoder_attention_mask\"] = tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "        return processed\n",
        "\n",
        "    dataset = dataset.map(\n",
        "        preprocess_function,\n",
        "        remove_columns=dataset.column_names\n",
        "    ).with_format(\"torch\")\n",
        "    dataloader = DataLoader(dataset, shuffle=(True if mode==\"train\" else False), batch_size=batch_size)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def jsonlload(fname):\n",
        "    with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().strip().split(\"\\n\")\n",
        "        j_list = [json.loads(line) for line in lines]\n",
        "\n",
        "    return j_list\n",
        "\n",
        "\n",
        "def jsonldump(j_list, fname):\n",
        "    with open(fname, \"w\", encoding='utf-8') as f:\n",
        "        for json_data in j_list:\n",
        "            f.write(json.dumps(json_data, ensure_ascii=False)+'\\n')\n"
      ]
    }
  ]
}